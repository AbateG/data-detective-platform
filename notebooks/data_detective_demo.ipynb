{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c40201",
   "metadata": {},
   "source": [
    "# Data Detective Platform - Exploratory Analysis\n",
    "\n",
    "This notebook demonstrates the key features of the Data Detective Platform, showcasing data analysis, anomaly detection, and debugging capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import networkx as nx\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7bd50",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40161b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample datasets\n",
    "employee_df = pd.read_csv('../data/sample_employee_data.csv')\n",
    "transaction_df = pd.read_csv('../data/sample_transaction_data.csv')\n",
    "anomaly_df = pd.read_csv('../data/sample_anomaly_data.csv')\n",
    "\n",
    "print(\"Employee Data Shape:\", employee_df.shape)\n",
    "print(\"Transaction Data Shape:\", transaction_df.shape)\n",
    "print(\"Anomaly Data Shape:\", anomaly_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83badd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"\\nEmployee Data Info:\")\n",
    "employee_df.info()\n",
    "\n",
    "print(\"\\nTransaction Data Info:\")\n",
    "transaction_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf03ea0",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment (Sanity Checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bcfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(employee_df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(transaction_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4764c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"Duplicate Rows:\")\n",
    "print(\"Employees:\", employee_df.duplicated().sum())\n",
    "print(\"Transactions:\", transaction_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nEmployee Salary Statistics:\")\n",
    "print(employee_df['salary'].describe())\n",
    "\n",
    "print(\"\\nTransaction Amount Statistics:\")\n",
    "print(transaction_df['amount'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfe1b3",
   "metadata": {},
   "source": [
    "## 3. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for anomaly detection\n",
    "numeric_cols = employee_df.select_dtypes(include=[np.number]).columns\n",
    "X = employee_df[numeric_cols]\n",
    "\n",
    "# Fit Isolation Forest\n",
    "clf = IsolationForest(contamination=0.1, random_state=42)\n",
    "employee_df['anomaly_score'] = clf.fit_predict(X)\n",
    "employee_df['anomaly'] = employee_df['anomaly_score'] == -1\n",
    "\n",
    "print(\"Anomalies detected:\", employee_df['anomaly'].sum())\n",
    "print(\"\\nAnomalous records:\")\n",
    "employee_df[employee_df['anomaly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomalies\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(employee_df['age'], employee_df['salary'], \n",
    "           c=employee_df['anomaly'].map({True: 'red', False: 'blue'}),\n",
    "           alpha=0.6)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Salary')\n",
    "plt.title('Employee Data - Anomalies Highlighted')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "anomaly_df['timestamp'] = pd.to_datetime(anomaly_df['timestamp'])\n",
    "plt.plot(anomaly_df['timestamp'], anomaly_df['value'], 'b-', alpha=0.7)\n",
    "anomalies = anomaly_df[abs(anomaly_df['value']) > 15]\n",
    "plt.scatter(anomalies['timestamp'], anomalies['value'], color='red', s=50)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Anomalies')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58447a94",
   "metadata": {},
   "source": [
    "## 4. Data Flow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample data flow graph\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([\n",
    "    ('Data Source', 'ETL Process'),\n",
    "    ('ETL Process', 'Data Warehouse'),\n",
    "    ('Data Warehouse', 'Analytics'),\n",
    "    ('Analytics', 'Reports'),\n",
    "    ('Data Warehouse', 'API'),\n",
    "    ('API', 'Applications')\n",
    "])\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "        node_size=2000, font_size=10, font_weight='bold',\n",
    "        arrows=True, arrowsize=20, edge_color='gray')\n",
    "plt.title('Sample Data Flow Architecture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811b9e0",
   "metadata": {},
   "source": [
    "## 5. Transaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639359e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transaction patterns\n",
    "transaction_df['timestamp'] = pd.to_datetime(transaction_df['timestamp'])\n",
    "transaction_df['hour'] = transaction_df['timestamp'].dt.hour\n",
    "\n",
    "# Check for suspicious transactions\n",
    "suspicious = transaction_df[\n",
    "    (transaction_df['amount'] > 1000) | \n",
    "    (transaction_df['amount'] < 0)\n",
    "]\n",
    "\n",
    "print(\"Suspicious Transactions:\")\n",
    "print(suspicious)\n",
    "\n",
    "# Transaction volume by hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "transaction_df.groupby('hour')['amount'].count().plot(kind='bar')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.title('Transaction Volume by Hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bd255",
   "metadata": {},
   "source": [
    "## 6. Log Analysis Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate log analysis\n",
    "log_data = \"\"\"\n",
    "2023-09-01 10:15:23 INFO User login successful\n",
    "2023-09-01 10:16:45 WARNING Invalid password attempt\n",
    "2023-09-01 10:17:12 INFO Data export completed\n",
    "2023-09-01 10:18:33 ERROR Database connection failed\n",
    "2023-09-01 10:19:01 INFO API request processed\n",
    "2023-09-01 10:20:15 WARNING Rate limit exceeded\n",
    "\"\"\"\n",
    "\n",
    "log_lines = [line.strip() for line in log_data.split('\\n') if line.strip()]\n",
    "log_levels = [line.split()[2] for line in log_lines if len(line.split()) >= 3]\n",
    "\n",
    "# Count log levels\n",
    "from collections import Counter\n",
    "level_counts = Counter(log_levels)\n",
    "\n",
    "print(\"Log Level Distribution:\")\n",
    "for level, count in level_counts.items():\n",
    "    print(f\"{level}: {count}\")\n",
    "\n",
    "# Visualize log levels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(level_counts.keys(), level_counts.values())\n",
    "plt.xlabel('Log Level')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Log Level Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eea66e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the core capabilities of the Data Detective Platform:\n",
    "\n",
    "1. **Data Quality Assessment**: Checking for missing values, duplicates, and statistical anomalies\n",
    "2. **Anomaly Detection**: Using machine learning to identify outliers in datasets\n",
    "3. **Data Flow Visualization**: Mapping data movement through systems\n",
    "4. **Transaction Analysis**: Identifying suspicious financial patterns\n",
    "5. **Log Analysis**: Processing and analyzing system logs for issues\n",
    "\n",
    "The platform integrates all these capabilities into an interactive web application for comprehensive data debugging and analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
